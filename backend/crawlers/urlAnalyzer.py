# Responsible for cleaning URL strings and fetching page contents using
# urllib. URLs are usually passed from crawler.py. pageStrings generated by
# urlAnalyzer.py are usually passed to htmlAnalyzer.py.

import urllib.request
import crawlers.htmlAnalyzer as ha


class ParseError(Exception):
    """ Exception for errors while parsing a link """
    pass


def clean_url(url):
    """ Add proper headings URLs for crawler analysis """
    # cast url to string
    urlString = str(url)
    if not ha.parsable(urlString):
        # check starts
        if urlString.startswith('http'):
            pass
        elif urlString.startswith("www"):
            urlString = "http://" + urlString
        else:
            urlString = "http://www." + urlString
    return urlString


def url_to_pageString(url, timeout=5):
    """ Cleans and converts string of URL link to string of page contents """
    # add proper headers to url
    cleanedURL = clean_url(url)
    try:
        # get response object of url, failing after timeout seconds
        page = urllib.request.urlopen(cleanedURL, timeout=timeout)
    except:
        raise ParseError(f"Unable to access '{cleanedURL}''")
    pageString = page.read()
    page.close()
    return(pageString)


def urlList_to_stringList(urlList):
    errors = 0
    stringList = []
    for count, url in enumerate(urlList):
        try:
            urlString = url_to_string(url)
            stringList.append(url_to_string(url))
        except:
            stringList.append("ERROR")
            errors += 1
        print(f"\t{count} urls analyzed with {errors} errors", end="\r")
    return stringList
